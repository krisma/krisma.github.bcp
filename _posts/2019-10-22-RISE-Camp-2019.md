---
title: "RISE Camp 2019"
date: 2019-10-22T21:49:30-07:00
categories:
  - blog
tags:
  - Python
  - ML
  - AI
---

Last Thursday (10/18) I attended the annual conference RISE Camp hosted by UC Berkeley RISELab. Since Microsoft is one of the major sponsors of RISELab, we get free tickets every year. RISELab has been focusing on â€œReal-time Intelligent Secure Explainable Systemsâ€ (thatâ€™s what RISE stands for). Itâ€™s one of my favorite conferences since 2017 because it includes hands-on lab exercises, in which we can use the framework they built to solve problems. The problems are organized in a series of jupyter notebooks that every attendee could clone and run right at the event.

One of their most interesting projects is called â€œRayâ€, which has 9K+ stars on Github by the time of this event. To summarize, Ray is a framework in Python which helps users to build and run distributed applications. During the coding session, I tried out the Ray python package to parallelize the run of a user-defined function using the feature called â€œremote functionâ€. It is smartly designed, such that I only need to add one python decorator right before the function I need to parallelize, and then call ray.get() on the result to actualize the run. 


# Install Dependencies

If you are running on Google Colab, you need to install the necessary dependencies before beginning the exercise.


```python
print('NOTE: Intentionally crashing session to use the newly installed library.\n')

!pip uninstall -y pyarrow
!pip install ray[debug]==0.7.5
!pip install bs4

# A hack to force the runtime to restart, needed to include the above dependencies.
import os
os._exit(0)
```

    NOTE: Intentionally crashing session to use the newly installed library.
    
    [33mWARNING: Skipping pyarrow as it is not installed.[0m
    Requirement already satisfied: ray[debug]==0.7.5 in /usr/local/lib/python3.6/dist-packages (0.7.5)
    Requirement already satisfied: redis>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.3.11)
    Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (2.6.0)
    Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.13)
    Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.16.5)
    Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (0.4.1)
    Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.10.0)
    Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.12.0)
    Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.6.4)
    Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.0.12)
    Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (7.0)
    Requirement already satisfied: funcsigs in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.0.2)
    Requirement already satisfied: setproctitle; extra == "debug" in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.1.10)
    Requirement already satisfied: py-spy; extra == "debug" in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (0.2.2)
    Requirement already satisfied: psutil; extra == "debug" in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (5.4.8)
    Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[debug]==0.7.5) (41.2.0)
    Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (19.2.0)
    Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (7.2.0)
    Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (0.7.1)
    Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.8.0)
    Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.3.0)
    Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)
    Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)
    

# Exercise 1 - Simple Parallelization

**GOAL:** The goal of this exercise is to show how to run simple tasks in parallel.

This script is running too slowly, but the computation is embarrassingly parallel. In this exercise, you will use Ray to execute the functions in parallel to speed it up.

### Introduction to Remote Functions

The `@ray.remote` decorator turns a Python function into a "remote function" that Ray can run in parallel. Here is a simple example:

```python
# A regular Python function.
def regular_function():
    return 1

# A Ray remote function.
@ray.remote
def remote_function():
    return 1
```

There are a few key differences between the original function and the decorated one:

1. **Invocation:** The regular version is called with `regular_function()`, whereas the remote version is called with `remote_function.remote()`.
2. **Return values:** `regular_function` executes synchronously and returns the result of the function (`1`), whereas `remote_function` immediately returns an `ObjectID` (a future) and then executes the task in the background on a separate worker process. The result of the future can be obtained by calling `ray.get` on the `ObjectID`.
    ```python
    >>> regular_function()
    1
    
    >>> remote_function.remote()
    ObjectID(1c80d6937802cd7786ad25e50caf2f023c95e350)
    
    >>> ray.get(remote_function.remote())
    1
    ```
3. **Parallelism:** Invocations of `regular_function` happen **serially**:
    ```python
    # These are executed one at a time, back-to-back.
    result = 0
    for _ in range(4):
        result += regular_function()
    assert result == 4
    ```
    In contrast, invocations of `remote_function` happen in **parallel**:
    ```python
    # Executing these functions happens at the same time in the background, and we get the results using ray.get.
    results = []
    for _ in range(4):
        results.append(remote_function.remote())
    assert sum(ray.get(results)) == 4
    ```


```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import ray
import time

print('Successfully imported ray!')
```

    Successfully imported ray!
    

Start the processes that make up the Ray runtime. By default, Ray does not schedule more tasks concurrently than there are CPUs, but this example requires four tasks to run concurrently, so we tell Ray that there are four CPUs. In practice, you would usually just let Ray detect the number of CPUs on the machine.

`ignore_reinit_error=True` just suppresses errors if the cell is run multiple times.


```python
ray.init(num_cpus=4, ignore_reinit_error=True)

# Sleep a little to improve the accuracy of the timing measurements used below,
# because some workers may still be starting up in the background.
time.sleep(2.0)
```

    2019-10-17 16:20:00,584	INFO resource_spec.py:205 -- Starting Ray with 6.3 GiB memory available for workers and up to 3.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
    

**EXERCISE:** The loop below takes too long, but the four function calls could be executed in parallel. This should speed up the execution from four seconds to one second. 

First, turn `slow_function` into a remote function, then execute the tasks in parallel by calling `slow_function.remote()` and obtaining the results with `ray.get` on the list of returned object IDs.


```python
# A function simulating a more interesting computation that takes one second.
@ray.remote
def slow_function(i):
    time.sleep(1)
    return i

start_time = time.time()

results = []
for i in range(4):
  results.append(slow_function.remote(i))
results = ray.get(results)
duration = time.time() - start_time
print('Executing the for loop took {:.3f} seconds.'.format(duration))
print('The results are:', results)
print('Run the next cell to check if the exercise was performed correctly.')
```

    Executing the for loop took 1.014 seconds.
    The results are: [0, 1, 2, 3]
    Run the next cell to check if the exercise was performed correctly.
    

**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass.


```python
assert results == [0, 1, 2, 3], 'Did you remember to call ray.get?'
assert duration < 1.1, ('The loop took {:.3f} seconds. This is too slow.'
                        .format(duration))
assert duration > 1, ('The loop took {:.3f} seconds. This is too fast.'
                      .format(duration))

print('Success! The example took {} seconds.'.format(duration))
```

    Success! The example took 1.013681173324585 seconds.
    

**EXERCISE:** Use the UI to view the task timeline and to verify that the four tasks were executed in parallel. You can do this as follows.

1. Run the following cell to generate a JSON file containing the profiling data.
2. Download the timeline file by right clicking on `exercise_1.json` in the **Files** tab in the navigator to the left, right clicking, and selecting  **"Download"**.
3. Enter **chrome://tracing** into the Chrome web browser, click on the **"Load"** button, and select the downloaded JSON file.

To navigate within the timeline:
- Move around by clicking and dragging.
- Zoom in and out by holding **alt** on Windows or **option** on Mac and scrolling.

**NOTE:** The timeline visualization will only work in **Chrome**.


```python
ray.timeline(filename="exercise_1.json")
```

# Exercise 2 - Parallel Data Processing with Task Dependencies

**GOAL:** The goal of this exercise is to pass object IDs into remote functions to encode dependencies between tasks.

In this exercise, we construct a sequence of tasks, each of which depends on the previous, mimicking a data parallel application. Within each sequence, tasks are executed serially, but multiple sequences of tasks can be executed in parallel. You will use Ray to speed up the computation by parallelizing the sequences.

### Concept for this Exercise - Task Dependencies

Consider the following basic remote function that returns the argument passed to it. If we pass in some normal Python objects, the results returned by `ray.get` should be the same objects.

```python
@ray.remote
def f(x):
    return x

>>> x1_id = f.remote(1)
>>> ray.get(x1_id)
1

>>> x2_id = f.remote([1, 2, 3])
>>> ray.get(x2_id)
[1, 2, 3]
```

However, **object IDs** can also be passed into remote functions. When the function is executed, Ray will automatically substitute the underlying Python object that the object ID refers to. In a sense, it's the same as calling `ray.get` on each argument that's passed in as an argument.

```python
>>> y1_id = f.remote(x1_id)
>>> ray.get(y1_id)
1

>>> y2_id = f.remote(x2_id)
>>> ray.get(y2_id)
[1, 2, 3]
```

When implementing a remote function, the function should expect a regular Python object regardless of whether the caller passes in a regular Python object or an object ID.

**These task dependencies affect scheduling.** In the example above, the task that creates `y1_id` depends on the task that creates `x1_id`. This means that:

- The second task will not be executed until the first task has finished executing.
- If the two tasks are scheduled on different machines, the output of the first task (the value corresponding to `x1_id`) will be copied over the network to the machine where the second task is scheduled.


```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import time

import ray
```


```python
ray.init(num_cpus=4, ignore_reinit_error=True)

# Sleep a little to improve the accuracy of the timing measurements used below,
# because some workers may still be starting up in the background.
time.sleep(2.0)
```

    2019-10-17 16:39:15,268	ERROR worker.py:1432 -- Calling ray.init() again after it has already been called.
    

**EXERCISE:** Below are some helper functions that mimic the pattern of a data parallel application that we want to speed up. To do so, you'll need to turn all of these functions into remote functions. Remember that you don't need to worry about whether the caller passes in an object ID or a regular object, because in both cases the arguments will be regular objects when the function executes. This means that even if you pass in an object ID, you **do not need to call `ray.get`** inside of these remote functions.


```python
@ray.remote
def load_data(filename):
    time.sleep(0.1)
    return np.ones((1000, 100))
@ray.remote
def normalize_data(data):
    time.sleep(0.1)
    return data - np.mean(data, axis=0)
@ray.remote
def extract_features(normalized_data):
    time.sleep(0.1)
    return np.hstack([normalized_data, normalized_data ** 2])
@ray.remote
def compute_loss(features):
    num_data, dim = features.shape
    time.sleep(0.1)
    return np.sum((np.dot(features, np.ones(dim)) - np.ones(num_data)) ** 2)

assert hasattr(load_data, 'remote'), 'load_data must be a remote function'
assert hasattr(normalize_data, 'remote'), 'normalize_data must be a remote function'
assert hasattr(extract_features, 'remote'), 'extract_features must be a remote function'
assert hasattr(compute_loss, 'remote'), 'compute_loss must be a remote function'
```

**EXERCISE:** The loop below takes too long. Parallelize the four passes through the loop by turning `load_data`, `normalize_data`, `extract_features`, and `compute_loss` into remote functions and then retrieving the losses with `ray.get`.

**NOTE:** You should only use **ONE** call to `ray.get`. For example, the object ID returned by `load_data` should be passed directly into `normalize_data` without needing to be retrieved by the driver.


```python
start_time = time.time()
losses = []
for filename in ['file1', 'file2', 'file3', 'file4']:
    inner_start = time.time()

    data = load_data.remote(filename)
    normalized_data = normalize_data.remote(data)
    features = extract_features.remote(normalized_data)
    loss = compute_loss.remote(features)
    losses.append(loss)
    
    inner_end = time.time()
    
    if inner_end - inner_start >= 0.1:
        raise Exception('You may be calling ray.get inside of the for loop! '
                        'Doing this will prevent parallelism from being exposed. '
                        'Make sure to only call ray.get once outside of the for loop.')

print('The losses are {}.'.format(losses) + '\n')
loss = sum(ray.get(losses))

duration = time.time() - start_time

print('The loss is {}. This took {:.3f} seconds. Run the next cell to see '
      'if the exercise was done correctly.'.format(loss, duration))
```

    The losses are [ObjectID(f3083a7ebdbaffffffff0100000000c001000000), ObjectID(5d294145d7bbffffffff0100000000c001000000), ObjectID(c2a79d56f2feffffffff0100000000c001000000), ObjectID(f0f55112a837ffffffff0100000000c001000000)].
    
    The loss is 4000.0. This took 0.483 seconds. Run the next cell to see if the exercise was done correctly.
    

**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass.


```python
assert loss == 4000
assert duration < 0.8, ('The loop took {:.3f} seconds. This is too slow.'
                        .format(duration))
assert duration > 0.4, ('The loop took {:.3f} seconds. This is too fast.'
                        .format(duration))

print('Success! The example took {:.3f} seconds.'.format(duration))
```

    Success! The example took 0.483 seconds.
    

**EXERCISE:** Use the UI to view the task timeline and to verify that the four tasks were executed in parallel. You can do this as follows.

1. Run the following cell to generate a JSON file containing the profiling data.
2. Download the timeline file by right clicking on `exercise_2.json` in the **Files** tab in the navigator to the left, right clicking, and selecting  **"Download"**.
3. Enter **chrome://tracing** into the Chrome web browser, click on the **"Load"** button, and select the downloaded JSON file.

To navigate within the timeline:
- Move around by clicking and dragging.
- Zoom in and out by holding **alt** on Windows or **option** on Mac and scrolling.

**NOTE:** The timeline visualization will only work in **Chrome**.


```python
ray.timeline(filename="exercise_2.json")
```

### Application: Parallel web-scraping

One useful application of what we have learned so far is to scrape information from the web. We will illustrate this in a toy setting, but the same principles apply on a large scale where crawling through websites, parsing them and extracting useful content (e.g. for building a search index or populating a database) is often very computationally demanding.

We break up the process into multiple steps. We first grab the raw HTML of the website using Python's requests package. Then, we use BeautifulSoup to parse the HTML to find the relevant information. Finally, we populate a pandas DataFrames so that we are able to work with the data.

To demonstrate this, we scrape GitHub commits to see the latest commits on several repositories.


```python
from bs4 import BeautifulSoup
import requests

import pandas as pd
```

The following function uses these libraries to parse the latest commits from several repositories on GitHub.


```python
@ray.remote
def fetch_commits(repo):
    url = 'https://github.com/{}/commits/master'.format(repo)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    df = pd.DataFrame(columns=['title', 'link'])
    for g in soup.find_all(class_='commit-title'):
        entry = {}
        title = g.find_all(class_='message')[0]['aria-label']
        entry['title'] = title
        links = g.find_all(class_='issue-link')
        if len(links) >= 1:
            link = links[0]['data-url']
            entry['link'] = link
        df = df.append(pd.DataFrame(entry, index=[0]), sort=False)
    
    df['repository'] = repo
    return df
```

Let's try this out to get results for some ray related topics serially.


```python
start = time.time()
repos = ["ray-project/ray", "modin-project/modin", "tensorflow/tensorflow", "apache/arrow"]
results = []
for repo in repos:
    df = fetch_commits.remote(repo)
    results.append(df)
    
df = pd.concat(ray.get(results), sort=False)
duration = time.time() - start
print("Constructing the dataframe took {:.3f} seconds.".format(duration))
```

    Constructing the dataframe took 1.753 seconds.
    

**EXERCISE**: Speed up the above serial query by making `fetch_commits` a remote function in order to scrape GitHub results in parallel. Then, see a sample of the data scraped below and feel free to play with the data to find other resources to learn more about these libraries!


```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>link</th>
      <th>repository</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Remove actor handle IDs (#5889)\n\n* Remove ac...</td>
      <td>https://github.com/ray-project/ray/issues/5889</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Fix typo in examples/centralized_critic.py (#5...</td>
      <td>https://github.com/ray-project/ray/issues/5943</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Add dependencies for dashboard to installation...</td>
      <td>https://github.com/ray-project/ray/issues/5942</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Update max resource label and give better erro...</td>
      <td>https://github.com/ray-project/ray/issues/5916</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[Autoscaler] Update AWS Deep Learning AMI to v...</td>
      <td>https://github.com/ray-project/ray/issues/5932</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Update TF documentation (#5918)</td>
      <td>https://github.com/ray-project/ray/issues/5918</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[tune] tf2.0 mnist example (#5898)\n\n* tfmnis...</td>
      <td>https://github.com/ray-project/ray/issues/5898</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Automatically create custom node id resource (...</td>
      <td>https://github.com/ray-project/ray/issues/5882</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[tune] Support TF2.0 on Keras Callback (#5912)</td>
      <td>https://github.com/ray-project/ray/issues/5912</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>remove evil redirects (#5919)</td>
      <td>https://github.com/ray-project/ray/issues/5919</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Deactivate bazel caching for linux wheels (#5915)</td>
      <td>https://github.com/ray-project/ray/issues/5915</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[tune] Explicitly set scheduler in run() (#587...</td>
      <td>https://github.com/ray-project/ray/issues/5871</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[docs] Pictures for all the Examples (#5859)\n...</td>
      <td>https://github.com/ray-project/ray/issues/5859</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Add back TensorFlow test (#5885)</td>
      <td>https://github.com/ray-project/ray/issues/5885</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[autoscaler] Worker-Head termination + Better ...</td>
      <td>https://github.com/ray-project/ray/issues/5909</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Bump dev version to 0.8.0.dev6 (#5906)</td>
      <td>https://github.com/ray-project/ray/issues/5906</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[tune] Remove TF MNIST example + add TrialRunn...</td>
      <td>https://github.com/ray-project/ray/issues/5868</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[tune] CPU-Only Head Node support (#5900)\n\n*...</td>
      <td>https://github.com/ray-project/ray/issues/5900</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Fix test_dying_worker_get (#5908)</td>
      <td>https://github.com/ray-project/ray/issues/5908</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[autoscaler] uptime redirect fix (#5907)\n\n* ...</td>
      <td>https://github.com/ray-project/ray/issues/5907</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[autoscaler] Revert to double-spawning updater...</td>
      <td>https://github.com/ray-project/ray/issues/5903</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[Serve] Remove handle passing in tail recursio...</td>
      <td>https://github.com/ray-project/ray/issues/5894</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>rllib: use pytorch's fn to see if gpu is avail...</td>
      <td>https://github.com/ray-project/ray/issues/5890</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[minor][docs] Remove example link (#5880)</td>
      <td>https://github.com/ray-project/ray/issues/5880</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Don't wrap RayError with RayTaskError (#5870)</td>
      <td>https://github.com/ray-project/ray/issues/5870</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[autoscaler] Fix quoting (#5891)</td>
      <td>https://github.com/ray-project/ray/issues/5891</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[Serve] Hotfix: Fix actor handle hashing in me...</td>
      <td>https://github.com/ray-project/ray/issues/5886</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Python 2 compatibility. (#5887)</td>
      <td>https://github.com/ray-project/ray/issues/5887</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Fix str of RayTaskError (#5878)\n\n* fix key e...</td>
      <td>https://github.com/ray-project/ray/issues/5878</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Fix linux wheel build (#5881)</td>
      <td>https://github.com/ray-project/ray/issues/5881</td>
      <td>ray-project/ray</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6898: [Java] Fix potential memory leak i...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6671: [C++][Python] Use more consistent ...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6862: [Developer] Check pull request tit...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6901: [Rust] [Parquet] Increment total_n...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6874: [Python] Fix memory leak when conv...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6903: [Python] Attempt to fix Python whe...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6876: [C++][Parquet] Use shared_ptr to a...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6847: [C++] Add range_expression adapter...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>[ARROW-6865][Java] Improve the performance of ...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6814: [C++] Resolve compiler warnings oc...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6853: [Java] Support vector and dictiona...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6659: [Rust] [DataFusion] Refactor of Ha...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6789: [Python] Improve ergonomics by aut...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6844: [C++][Parquet] Fix regression in r...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6885: [Python] Remove superfluous skippe...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6567: [Rust] [DataFusion] Wrap aggregate...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6452: [Java] Override ValueVector toStri...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6184: [Java] Provide hash table based di...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6877: [C++] Add additional Boost version...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6882: [C++] Ensure the DictionaryArray i...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6283: [Rust] [DataFusion] Implement Cont...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-4219: [Rust] [Parquet] Initial support f...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6857: [C++] Fix DictionaryEncode for zer...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6873: [Python]Â Remove stale CColumn refe...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6852: [C++] Fix build issue on memory-be...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6864: [C++] Add compression-related comp...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-4748: [Rust] [DataFusion] Optimize GROUP...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6860: [Python][C++] Do not link shared l...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-5680: [Rust] [DataFusion] GROUP BY sql t...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
    <tr>
      <th>0</th>
      <td>ARROW-6690: [Rust] [DataFusion] Optimize aggre...</td>
      <td>NaN</td>
      <td>apache/arrow</td>
    </tr>
  </tbody>
</table>
<p>140 rows Ã— 3 columns</p>
</div>



# Exercise 3 - Nested Parallelism

**GOAL:** The goal of this exercise is to show how to create nested tasks by calling a remote function inside of another remote function.

In this exercise, you will implement the structure of a parallel hyperparameter sweep which trains a number of models in parallel. Each model will be trained using parallel gradient computations.

### Concepts for this Exercise - Nested Remote Functions

Remote functions can call other functions. For example, consider the following.

```python
@ray.remote
def f():
    return 1

@ray.remote
def g():
    # Call f 4 times and return the resulting object IDs.
    results = []
    for _ in range(4):
      results.append(f.remote())
    return results

@ray.remote
def h():
    # Call f 4 times, block until those 4 tasks finish,
    # retrieve the results, and return the values.
    results = []
    for _ in range(4):
      results.append(f.remote())
    return ray.get(results)
```

Then calling `g` and `h` produces the following behavior.

```python
>>> ray.get(g.remote())
[ObjectID(b1457ba0911ae84989aae86f89409e953dd9a80e),
 ObjectID(7c14a1d13a56d8dc01e800761a66f09201104275),
 ObjectID(99763728ffc1a2c0766a2000ebabded52514e9a6),
 ObjectID(9c2f372e1933b04b2936bb6f58161285829b9914)]

>>> ray.get(h.remote())
[1, 1, 1, 1]
```


```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import time

import ray
```


```python
ray.init(num_cpus=9, ignore_reinit_error=True)

# Sleep a little to improve the accuracy of the timing measurements used below,
# because some workers may still be starting up in the background.
time.sleep(2.0)
```

    2019-10-17 17:01:37,951	ERROR worker.py:1432 -- Calling ray.init() again after it has already been called.
    

This example represents a hyperparameter sweep in which multiple models are trained in parallel. Each model training task also performs data parallel gradient computations.

**EXERCISE:** Turn `compute_gradient` and `train_model` into remote functions so that they can be executed in parallel. Inside of `train_model`, do the calls to `compute_gradient` in parallel and fetch the results using `ray.get`.


```python
@ray.remote
def compute_gradient(data, current_model):
    time.sleep(0.03)
    return 1
@ray.remote
def train_model(hyperparameters):
    current_model = 0
    # Iteratively improve the current model. This outer loop cannot be parallelized.
    for _ in range(10):
        # EXERCISE: Parallelize the list comprehension in the line below. After you
        # turn "compute_gradient" into a remote function, you will need to call it
        # with ".remote". The results must be retrieved with "ray.get" before "sum"
        # is called.
        total_gradient = sum(ray.get([compute_gradient.remote(j, current_model) for j in range(2)]))
        current_model += total_gradient

    return current_model

assert hasattr(compute_gradient, 'remote'), 'compute_gradient must be a remote function'
assert hasattr(train_model, 'remote'), 'train_model must be a remote function'
```

**EXERCISE:** The code below runs 3 hyperparameter experiments. Change this to run the experiments in parallel.


```python
# Sleep a little to improve the accuracy of the timing measurements below.
time.sleep(2.0)
start_time = time.time()

# Run some hyperparaameter experiments.
results = []
for hyperparameters in [{'learning_rate': 1e-1, 'batch_size': 100},
                        {'learning_rate': 1e-2, 'batch_size': 100},
                        {'learning_rate': 1e-3, 'batch_size': 100}]:
    results.append(train_model.remote(hyperparameters))
results = ray.get(results)
# EXERCISE: Once you've turned "results" into a list of Ray ObjectIDs
# by calling train_model.remote, you will need to turn "results" back
# into a list of integers, e.g., by doing "results = ray.get(results)".

end_time = time.time()
duration = end_time - start_time

assert all([isinstance(x, int) for x in results]), \
    'Looks like "results" is {}. You may have forgotten to call ray.get.'.format(results)
```

**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass.

**NOTE:** This exercise is known to have issues running on remote notebooks that can be resolved by rerunning the above cell a second time.


```python
assert results == [20, 20, 20]
assert duration < 0.5, ('The experiments ran in {:.3f} seconds. This is too '
                         'slow.'.format(duration))
assert duration > 0.3, ('The experiments ran in {:.3f} seconds. This is too '
                        'fast.'.format(duration))

print('Success! The example took {:.3f} seconds.'.format(duration))
```

    Success! The example took 0.492 seconds.
    

**EXERCISE:** Use the UI to view the task timeline and to verify that the four tasks were executed in parallel. You can do this as follows.

1. Run the following cell to generate a JSON file containing the profiling data.
2. Download the timeline file by right clicking on `exercise_3.json` in the **Files** tab in the navigator to the left, right clicking, and selecting  **"Download"**.
3. Enter **chrome://tracing** into the Chrome web browser, click on the **"Load"** button, and select the downloaded JSON file.

To navigate within the timeline:
- Move around by clicking and dragging.
- Zoom in and out by holding **alt** on Windows or **option** on Mac and scrolling.

**NOTE:** The timeline visualization will only work in **Chrome**.


```python
ray.timeline(filename="exercise_3.json")
```
